# syntax=docker/dockerfile:1
# check=skip=InvalidDefaultArgInFrom # defaults are specified in bakefiles
# vim: ft=dockerfile

ARG BASE_UBI_IMAGE_TAG
ARG PYTHON_VERSION

## Base Layer ##################################################################
FROM registry.access.redhat.com/ubi9/ubi-minimal:${BASE_UBI_IMAGE_TAG} AS base
ARG PYTHON_VERSION
ENV PYTHON_VERSION=${PYTHON_VERSION}
RUN microdnf -y update && microdnf install -y \
    python${PYTHON_VERSION}-pip python${PYTHON_VERSION}-wheel \
    && microdnf clean all

WORKDIR /workspace

ENV LANG=C.UTF-8 \
    LC_ALL=C.UTF-8

# Some utils for dev purposes - tar required for kubectl cp
RUN microdnf install -y \
        which procps findutils tar \
    && microdnf clean all


## Python Installer ############################################################
FROM base AS python-install
ARG PYTHON_VERSION
ARG CUDA_MAJOR
ARG CUDA_MINOR

ENV CUDA_MAJOR=${CUDA_MAJOR}
ENV CUDA_MINOR=${CUDA_MINOR}
ENV UV_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cu${CUDA_MAJOR}${CUDA_MINOR}
ENV UV_INDEX_STRATEGY=unsafe-best-match
ENV PIP_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cu${CUDA_MAJOR}${CUDA_MINOR}

ENV VIRTUAL_ENV=/opt/vllm
ENV PATH="$VIRTUAL_ENV/bin:$PATH"
ENV PYTHON_VERSION=${PYTHON_VERSION}
RUN microdnf install -y \
    python${PYTHON_VERSION}-devel  && \
    python${PYTHON_VERSION} -m venv $VIRTUAL_ENV && \
    pip install --no-cache -U pip wheel uv && \
    microdnf clean all


## Release #####################################################################
FROM python-install AS vllm-openai
ARG PYTHON_VERSION

WORKDIR /workspace

ENV VIRTUAL_ENV=/opt/vllm
ENV PATH=$VIRTUAL_ENV/bin:$PATH

# force using the python venv's cuda runtime libraries
ENV LD_LIBRARY_PATH="${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/nvidia/cuda_nvrtc/lib:${LD_LIBRARY_PATH}"
ENV LD_LIBRARY_PATH="${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/nvidia/cuda_runtime/lib:${LD_LIBRARY_PATH}"
ENV LD_LIBRARY_PATH="${VIRTUAL_ENV}/lib/python${PYTHON_VERSION}/site-packages/nvidia/nvtx/lib:${LD_LIBRARY_PATH}"

# Triton needs a CC compiler
RUN microdnf install -y gcc && \
    microdnf clean all


COPY LICENSE /licenses/vllm.md
COPY examples/*.jinja /app/data/template/

# install vllm (and any other dependencies )by running the payload script
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,src=payload,target=/workspace/payload \
    ./payload/run.sh


ENV HF_HUB_OFFLINE=1 \
    HOME=/home/vllm \
    # Allow requested max length to exceed what is extracted from the
    # config.json
    # see: https://github.com/vllm-project/vllm/pull/7080
    VLLM_ALLOW_LONG_MAX_MODEL_LEN=1 \
    VLLM_USAGE_SOURCE=production-docker-image \
    VLLM_WORKER_MULTIPROC_METHOD=fork \
    VLLM_NO_USAGE_STATS=1 \
    OUTLINES_CACHE_DIR=/tmp/outlines \
    NUMBA_CACHE_DIR=/tmp/numba \
    TRITON_CACHE_DIR=/tmp/triton \
    # Setup NCCL monitoring with torch
    # For tensor-parallel workloads, this monitors for NCCL deadlocks when
    # one rank dies, and tears down the NCCL process groups so that the driver
    # can cleanly exit.
    TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC=15 \
    TORCH_NCCL_DUMP_ON_TIMEOUT=0

# setup non-root user for OpenShift
RUN umask 002 && \
    useradd --uid 2000 --gid 0 vllm && \
    mkdir -p /home/vllm && \
    chmod g+rwx /home/vllm

USER 2000
WORKDIR /home/vllm

ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]


## TGIS Adapter layer #####################################################################
FROM vllm-openai AS vllm-grpc-adapter

USER root

ARG VLLM_TGIS_ADAPTER_VERSION
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,src=payload,target=/workspace/payload \
    cd /workspace && \
    env HOME=/root VLLM_TGIS_ADAPTER_VERSION=${VLLM_TGIS_ADAPTER_VERSION} \
        ./payload/run.sh

ENV GRPC_PORT=8033 \
    PORT=8000 \
    # As an optimization, vLLM disables logprobs when using spec decoding by
    # default, but this would be unexpected to users of a hosted model that
    # happens to have spec decoding
    # see: https://github.com/vllm-project/vllm/pull/6485
    DISABLE_LOGPROBS_DURING_SPEC_DECODING=false

USER 2000
ENTRYPOINT ["python3", "-m", "vllm_tgis_adapter", "--uvicorn-log-level=warning"]
